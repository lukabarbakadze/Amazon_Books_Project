{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions to Extract Data from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.amazon.com/Data-Science-Business-Data-Analytic-Thinking/dp/B08VL5K5ZX/ref=sr_1_13?crid=YS0ZZ9RZH5I9&keywords=data+analytics+books&qid=1673030834&sprefix=data+analytics+books%2Caps%2C307&sr=8-13\"\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "# page = requests.get(url, headers=headers)\n",
    "\n",
    "# soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find(\"span\", attrs={\"id\":'productTitle'}).get_text().strip()\n",
    "        return title\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        title = soup.find(id=\"title\").get_text().strip()\n",
    "        return title\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = soup.find(id=\"price\").get_text().strip()\n",
    "        return price\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        price = soup.find(class_=\"a-color-price\").get_text().strip()\n",
    "        return price\n",
    "    except:\n",
    "        \"\"\n",
    "\n",
    "def get_author(soup):\n",
    "    try:\n",
    "        authors_ls = soup.find(id=\"followTheAuthor_feature_div\").find_all(class_=\"a-size-base a-link-normal a-text-normal\")\n",
    "        authors_ls = [author.get_text().strip() for author in authors_ls]\n",
    "        return authors_ls\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        authors_ls = re.sub('\\s+',' ', soup.find(id=\"audibleProductTitle_byline\").get_text())\n",
    "        return [authors_ls]\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_product_details(soup):\n",
    "    try:\n",
    "        ls = soup.find(id=\"detailBullets_feature_div\").get_text().replace(\"\\n\", \"\").replace(\"  \", \"\").replace(\"\\u200f:\\u200e\",\"\").split(\" \")\n",
    "    except:\n",
    "        return \"\", \"\"\n",
    "    try:\n",
    "        language_idx = ls.index(\"Language\")\n",
    "        language = ls[language_idx+1]\n",
    "    except: language = \"\"\n",
    "    try:\n",
    "        page_idx = ls.index(\"pages\")\n",
    "        pages = ls[page_idx-1]\n",
    "    except: pages = \"\"\n",
    "    return language, pages\n",
    "\n",
    "def get_costumer_ratings(soup):\n",
    "    try:\n",
    "\n",
    "        overall_rating = soup.find(class_=\"a-fixed-left-grid-col aok-align-center a-col-right\").get_text().strip()\n",
    "        star_ratios = [rating.get_text().strip() for rating in soup.find(id=\"histogramTable\").find_all(class_=\"a-text-right a-nowrap\")]\n",
    "        return overall_rating, star_ratios\n",
    "    except:\n",
    "        return \"\", \"\"\n",
    "\n",
    "def get_total_review(soup):\n",
    "    try:\n",
    "        reviews = soup.find(class_=\"a-row a-spacing-medium averageStarRatingNumerical\").get_text().strip()\n",
    "        return reviews\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        reviews = soup.find(id=\"detailBullets_averageCustomerReviews\").get_text().strip()\n",
    "        return reviews\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_costumer_comments(soup):\n",
    "    try:\n",
    "        comments = [comment.get_text().strip() for comment in soup.find_all(\"span\", {\"data-hook\": \"review-body\"})]\n",
    "        return comments\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_books_links(soup):\n",
    "    books = soup.find(class_=\"s-main-slot s-result-list s-search-results sg-row\").find_all(class_=\"a-link-normal s-no-outline\")\n",
    "    links = []\n",
    "    for book in books:\n",
    "        links.append(\"https://www.amazon.com/\" + book[\"href\"])\n",
    "    return links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Loops"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Want to scrape Amazon books data about following topics**:\n",
    "- Data Science           (Search word: \"data science books\")\n",
    "- Data Analytics         (Search word: \"data analytics books\")\n",
    "- Machine Learning       (Search word: \"machine learning books\")\n",
    "- Data Engineering       (Search word: \"data engineering books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary to store data\n",
    "data_science = {}\n",
    "# scrape data\n",
    "# each page consists 60 books, so let's get first five page\n",
    "url = \"https://www.amazon.com/s?k=data+science+books&crid=N6CUDUWC74ES&qid=1674379592&sprefix=data+science+books%2Caps%2C902&ref=sr_pg_1\"\n",
    "book_id = 0\n",
    "pbar_page = tqdm(position=0, desc='Page bar', total = 3)\n",
    "pbar_book = tqdm(position=1, desc='Book bar', total = 150, leave=True)\n",
    "for page_number in range(2,6,1):                                            # loop for number of pages\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    soup = BeautifulSoup(soup.prettify(), \"html.parser\")                    # get page html\n",
    "    book_links = get_books_links(soup)                                      # get all book urls from whole page\n",
    "    for book_link in book_links:                                            # iterate through each book\n",
    "        book_page = requests.get(book_link, headers=headers)\n",
    "        book_soup = BeautifulSoup(book_page.content, \"html.parser\")\n",
    "        book_soup = BeautifulSoup(book_soup.prettify(), \"html.parser\")      # get single book's html\n",
    "        title = get_title(book_soup)                                        # get data\n",
    "        price = get_price(book_soup)\n",
    "        authors = get_author(book_soup)\n",
    "        language, pages = get_product_details(book_soup)\n",
    "        costumer_ratings = get_costumer_ratings(book_soup)\n",
    "        reviews = get_total_review(book_soup)\n",
    "        comments = get_costumer_comments(book_soup)\n",
    "        data_science[book_id] = {\"Title\": title,                            # save scrapped data into dictionary\n",
    "                         \"Authors\": authors,\n",
    "                         \"Price\": price,\n",
    "                         \"Language\": language,\n",
    "                         \"NumOfPages\": pages,\n",
    "                         \"CostumerRatings\": costumer_ratings,\n",
    "                         \"NumOfReviews\": reviews,\n",
    "                         \"BookLink\": book_link,\n",
    "                         \"Comments\": comments}\n",
    "        book_id+=1\n",
    "        pbar_book.update(1)\n",
    "        if book_id>300:\n",
    "            print(\"Scraping data from site is done!\")\n",
    "            break\n",
    "    if book_id>300:\n",
    "        break\n",
    "    page_label = f\"Go to page {page_number}\"\n",
    "    url = \"https://www.amazon.com/\" + soup.find(attrs={\"aria-label\": page_label})[\"href\"]\n",
    "    pbar_page.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analytics = {}\n",
    "url = \"https://www.amazon.com/s?k=data+analytics+books&crid=YS0ZZ9RZH5I9&qid=1674379655&sprefix=data+analytics+books%2Caps%2C307&ref=sr_pg_1\"\n",
    "book_id = 0\n",
    "pbar_page = tqdm(position=0, desc='Page bar', total = 3)\n",
    "pbar_book = tqdm(position=1, desc='Book bar', total = 150, leave=True)\n",
    "for page_number in range(2, 6, 1):                                            # loop for number of pages\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    soup = BeautifulSoup(soup.prettify(), \"html.parser\")                    # get page html\n",
    "    book_links = get_books_links(soup)                                      # get all book urls from whole page\n",
    "    for book_link in book_links:                                            # iterate through each book\n",
    "        book_page = requests.get(book_link, headers=headers)\n",
    "        book_soup = BeautifulSoup(book_page.content, \"html.parser\")\n",
    "        book_soup = BeautifulSoup(book_soup.prettify(), \"html.parser\")      # get single book's html\n",
    "        title = get_title(book_soup)                                        # get data\n",
    "        price = get_price(book_soup)\n",
    "        authors = get_author(book_soup)\n",
    "        language, pages = get_product_details(book_soup)\n",
    "        costumer_ratings = get_costumer_ratings(book_soup)\n",
    "        reviews = get_total_review(book_soup)\n",
    "        comments = get_costumer_comments(book_soup)\n",
    "        data_analytics[book_id] = {\"Title\": title,\n",
    "                         \"Authors\": authors,\n",
    "                         \"Price\": price,\n",
    "                         \"Language\": language,\n",
    "                         \"NumOfPages\": pages,\n",
    "                         \"CostumerRatings\": costumer_ratings,\n",
    "                         \"NumOfReviews\": reviews,\n",
    "                         \"BookLink\": book_link,\n",
    "                         \"Comments\": comments}\n",
    "        book_id+=1\n",
    "        pbar_book.update(1)\n",
    "        if book_id>300:\n",
    "            print(\"Scraping data from site is done!\")\n",
    "            break\n",
    "    if book_id>300:\n",
    "        break\n",
    "    page_label = f\"Go to page {page_number}\"\n",
    "    url = \"https://www.amazon.com/\" + soup.find(attrs={\"aria-label\": page_label})[\"href\"]\n",
    "    pbar_page.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_learning = {}\n",
    "url = \"https://www.amazon.com/s?k=machine+learning+books&crid=3QIJGWAEPCFZC&qid=1674379714&sprefix=machine+learningbooks%2Caps%2C267&ref=sr_pg_1\"\n",
    "book_id = 0\n",
    "pbar_page = tqdm(position=0, desc='Page bar', total = 3)\n",
    "pbar_book = tqdm(position=1, desc='Book bar', total = 150, leave=True)\n",
    "for page_number in range(2,6,1):                                            # loop for number of pages\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    soup = BeautifulSoup(soup.prettify(), \"html.parser\")                    # get page html\n",
    "    book_links = get_books_links(soup)                                      # get all book urls from whole page\n",
    "    for book_link in book_links:                                            # iterate through each book\n",
    "        book_page = requests.get(book_link, headers=headers)\n",
    "        book_soup = BeautifulSoup(book_page.content, \"html.parser\")\n",
    "        book_soup = BeautifulSoup(book_soup.prettify(), \"html.parser\")      # get single book's html\n",
    "        title = get_title(book_soup)                                        # get data\n",
    "        price = get_price(book_soup)\n",
    "        authors = get_author(book_soup)\n",
    "        language, pages = get_product_details(book_soup)\n",
    "        costumer_ratings = get_costumer_ratings(book_soup)\n",
    "        reviews = get_total_review(book_soup)\n",
    "        comments = get_costumer_comments(book_soup)\n",
    "        machine_learning[book_id] = {\"Title\": title,\n",
    "                         \"Authors\": authors,\n",
    "                         \"Price\": price,\n",
    "                         \"Language\": language,\n",
    "                         \"NumOfPages\": pages,\n",
    "                         \"CostumerRatings\": costumer_ratings,\n",
    "                         \"NumOfReviews\": reviews,\n",
    "                         \"BookLink\": book_link,\n",
    "                         \"Comments\": comments}\n",
    "        book_id+=1\n",
    "        pbar_book.update(1)\n",
    "        if book_id>300:\n",
    "            print(\"Scraping data from site is done!\")\n",
    "            break\n",
    "    if book_id>300:\n",
    "        break\n",
    "    page_label = f\"Go to page {page_number}\"\n",
    "    url = \"https://www.amazon.com/\" + soup.find(attrs={\"aria-label\": page_label})[\"href\"]\n",
    "    pbar_page.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_engineering = {}\n",
    "# here each page consists 22 books, so I'll iterate through more pages than other cases\n",
    "url = \"https://www.amazon.com/s?k=data+engineering+books&crid=23GGSOIXPQKBV&qid=1674379745&sprefix=data+engineering+books%2Caps%2C261&ref=sr_pg_1\"\n",
    "book_id = 0\n",
    "pbar_page = tqdm(position=0, desc='Page bar', total = 3)\n",
    "pbar_book = tqdm(position=1, desc='Book bar', total = 150, leave=True)\n",
    "for page_number in range(2,20,1):                                            # loop for number of pages\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    soup = BeautifulSoup(soup.prettify(), \"html.parser\")                    # get page html\n",
    "    book_links = get_books_links(soup)                                      # get all book urls from whole page\n",
    "    for book_link in book_links:                                            # iterate through each book\n",
    "        book_page = requests.get(book_link, headers=headers)\n",
    "        book_soup = BeautifulSoup(book_page.content, \"html.parser\")\n",
    "        book_soup = BeautifulSoup(book_soup.prettify(), \"html.parser\")      # get single book's html\n",
    "        title = get_title(book_soup)                                        # get data\n",
    "        price = get_price(book_soup)\n",
    "        authors = get_author(book_soup)\n",
    "        language, pages = get_product_details(book_soup)\n",
    "        costumer_ratings = get_costumer_ratings(book_soup)\n",
    "        reviews = get_total_review(book_soup)\n",
    "        comments = get_costumer_comments(book_soup)\n",
    "        data_engineering[book_id] = {\"Title\": title,\n",
    "                         \"Authors\": authors,\n",
    "                         \"Price\": price,\n",
    "                         \"Language\": language,\n",
    "                         \"NumOfPages\": pages,\n",
    "                         \"CostumerRatings\": costumer_ratings,\n",
    "                         \"NumOfReviews\": reviews,\n",
    "                         \"BookLink\": book_link,\n",
    "                         \"Comments\": comments}\n",
    "        book_id+=1\n",
    "        pbar_book.update(1)\n",
    "        if book_id>300:\n",
    "            print(\"Scraping data from site is done!\")\n",
    "            break\n",
    "    if book_id>300:\n",
    "        break\n",
    "    page_label = f\"Go to page {page_number}\"\n",
    "    url = \"https://www.amazon.com/\" + soup.find(attrs={\"aria-label\": page_label})[\"href\"]\n",
    "    pbar_page.update(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe & Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets dictionaries from scraping loops, creates dataframe, do little cleaning and saves it\n",
    "def dataframe_saver(data_science, data_analytics, machine_learning, data_engineering):\n",
    "    df_ds = pd.DataFrame(data_science).T\n",
    "    df[\"search_word\"] = \"Data Science\"\n",
    "    df_da = pd.DataFrame(data_analytics).T\n",
    "    df[\"search_word\"] = \"Data Analytics\"\n",
    "    df_ml = pd.DataFrame(machine_learning).T\n",
    "    df[\"search_word\"] = \"Machine Learning\"\n",
    "    df_de = pd.DataFrame(data_engineering).T\n",
    "    df[\"search_word\"] = \"Data Engineering\"\n",
    "    df = pd.concat([df_ds, df_da, df_ml, df_de])\n",
    "\n",
    "    ls = [\"Title\", \"Authors\", \"CostumerRatings\"]                                     # turn list's into string\n",
    "    for columns in ls:\n",
    "        df[columns] = df[columns].apply(lambda x: \"  \".join(map(str, x)))\n",
    "    columns = list(df.columns)                                                       # replace \"\" with np.nan\n",
    "    for column in columns:\n",
    "        df.loc[df[column]==\"\"][column] = np.nan\n",
    "    df[\"Language\"] = df['Language'].fillna(\"English\")                                # replace null values with \"English\" as all others have \"English\"\n",
    "    df.to_excel(\"Data\\data.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "136ac6390a149cc93e6470a8f7f2b930c2fa00ca17cae58650efc746ba1de66d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
